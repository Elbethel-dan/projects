{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8E11COILAjP",
    "outputId": "1f8f85c9-d522-4d42-f51c-a1aada3b0505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUsSUgmN6MeZ",
    "outputId": "bbe7fca6-0b65-477e-d9ce-19faae7ebeab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MM-euoy6b3u",
    "outputId": "d6d361cc-2679-477e-a558-5c1596587c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=f0eca84ce81cdf405811bee9822fffd7cfe91acda9c5e9dd8e015e08bf5f5665\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, seqeval, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JBiHGO26fTp"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzAeloM76i5r",
    "outputId": "8c21af1f-a7c7-444e-fddc-f1d30242c0ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence: ['ገለ', 'መሰኻኽር', 'ሓያሎ', 'ጠያይት', 'ክትኮስ', 'ከምዝሰምዑ', 'ክገለጹ', 'እንከለዉ', 'ሓደ', 'ናይ', 'ዓይኒ', 'ምስክር', 'ድማ', 'ኦቶማቲክ', 'ብረት', 'ዝሓዘ', 'ሰብ', 'ክትኩስ', 'ከምዝራኣየ', 'ተዛሪቡ', '።']\n",
      "First Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def load_conll_format(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Blank line indicates end of a sentence\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(ner_tags)\n",
    "                    tokens = []\n",
    "                    ner_tags = []\n",
    "            else:\n",
    "                token, tag = line.split()  # Split token and tag\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(tag)\n",
    "        # Add the last sentence if it exists\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(ner_tags)\n",
    "    return sentences, labels\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/content/drive/MyDrive/Tigrigna-NER/Tigrinya-NER-Dataset.txt\"  # Replace with your file's path\n",
    "sentences, ner_tags = load_conll_format(file_path)\n",
    "\n",
    "# Example output\n",
    "print(\"First Sentence:\", sentences[0])\n",
    "print(\"First Tags:\", ner_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wjyCB76A6o_t",
    "outputId": "926bb899-af7e-49ec-d119-e6acf6b45b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Sentence: ['ነቲ', 'ኣብ', 'ከተማ', 'ኢንዲያናፖሰስ', 'ዝተገብረ', 'ጃምላዊ', 'ቅትለት', 'ከምዝፈጸመ', 'ዝንገረሉ', 'ዘሎ', 'ሰብ', 'በይኑ', 'ዝፈጸሞ', 'ከይኮነ', 'ከምዘይተርፍ', 'ጸብጻባት', 'ሓቢሮም', '።']\n",
      "Second Tags: ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second Sentence:\", sentences[1])\n",
    "print(\"Second Tags:\", ner_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We2MbA1e6p85",
    "outputId": "9b05d881-6f5d-41b3-8f9b-e5e94325a3aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['ገለ', 'መሰኻኽር', 'ሓያሎ', 'ጠያይት', 'ክትኮስ', 'ከምዝሰምዑ', 'ክገለጹ', 'እንከለዉ', 'ሓደ', 'ናይ', 'ዓይኒ', 'ምስክር', 'ድማ', 'ኦቶማቲክ', 'ብረት', 'ዝሓዘ', 'ሰብ', 'ክትኩስ', 'ከምዝራኣየ', 'ተዛሪቡ', '።'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Prepare data for Hugging Face\n",
    "data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(sentences, ner_tags)]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_list(data)\n",
    "\n",
    "# Check the first few examples\n",
    "print(hf_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "a84bb2ee72bb4e98ba37d7a7693e3c96",
      "685df89a68964369a5c4e31dec5d579c",
      "b6b8cbec4fe141d084480c598b8723d6",
      "44d7dd9c082845eea582dd90c8c03b33",
      "3c54bf8de7cb444e8767ca237ba584b2",
      "3f0c5e40e3a4450f8454398874b6aa9c",
      "3f9d1857f2a449b0bb9f98ff29ec3e4c",
      "a5df2559619d4955a27f4765a8f536af",
      "c9befffb09754f39a0d353e2b2502086",
      "5d083ad03e4b49ddaf0adc2af73f2b6e",
      "7b6697af9fc0410792fa46d49a4128de"
     ]
    },
    "id": "mG1G1UiO6uUG",
    "outputId": "5369c3f2-b2a7-47de-8311-1a0a6a609032"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84bb2ee72bb4e98ba37d7a7693e3c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['ገለ', 'መሰኻኽር', 'ሓያሎ', 'ጠያይት', 'ክትኮስ', 'ከምዝሰምዑ', 'ክገለጹ', 'እንከለዉ', 'ሓደ', 'ናይ', 'ዓይኒ', 'ምስክር', 'ድማ', 'ኦቶማቲክ', 'ብረት', 'ዝሓዘ', 'ሰብ', 'ክትኩስ', 'ከምዝራኣየ', 'ተዛሪቡ', '።'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner_tags_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Define label-to-ID mapping (update with actual tags in your dataset)\n",
    "label2id = {\"O\": 0, \"B-LOC\": 1, \"I-LOC\": 2, \"B-PER\": 3, \"I-PER\": 4, \"B-ORG\": 5, \"I-ORG\": 6, \"B-DATE\":7, \"I-DATE\":8, \"B-MISC\":9, \"I-MISC\":10}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# Add label IDs to the dataset\n",
    "hf_dataset = hf_dataset.map(lambda x: {\"ner_tags_ids\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "\n",
    "# Inspect updated dataset\n",
    "print(hf_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYTb4O0x6w9i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split sentences and tags into train (80%), validation (10%), and test (10%)\n",
    "train_sentences, temp_sentences, train_tags, temp_tags = train_test_split(\n",
    "    sentences, ner_tags, test_size=0.2, random_state=42\n",
    ")\n",
    "val_sentences, test_sentences, val_tags, test_tags = train_test_split(\n",
    "    temp_sentences, temp_tags, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(train_sentences, train_tags)]\n",
    "val_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(val_sentences, val_tags)]\n",
    "test_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(test_sentences, test_tags)]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV0EDnC76zsy",
    "outputId": "ae4f6346-31e5-4dc6-b9d2-9757687b3864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 4562\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 570\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 571\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Combine splits into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "# Inspect the splits\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnRWvuPN62U5"
   },
   "outputs": [],
   "source": [
    "# Access the splits directly from the dataset_dict\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"validation\"]\n",
    "test_dataset = dataset_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VivIy8Ju66OP",
    "outputId": "1f3f5d25-81c7-4cbb-c357-a1c61f35cf7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train example: {'tokens': ['ኤል', 'ሳዳዊ', 'ወለዳ', 'ጓል', '10', 'ዓመት', 'እንከላ', 'ከመርዑውዋ', 'ፈቲኖም', 'ሓንጊዳ', 'ክትኣብዮም', 'እንከላ', 'ኣዲኣ', 'ኣብ', 'ጎና', 'ኮይና', 'ደጊፋታ', '።'], 'ner_tags': ['B-ORG', 'I-ORG', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "First validation example: {'tokens': ['ካብዚ', 'ሓሊፉ', 'ፕሮጀክት', 'ሪድ', 'ቱ', 'ምስ', 'ትካል', 'ገባሪ', 'ሰናይ', 'ክሪኤቲቭ', 'ኣሶሼት', 'ኢንተርናሽናል', 'ብምትሕብባር', 'ካብ', 'ወርሒ', 'መጋቢት', '2011', 'ዓ/ም', 'ጀሚሩ', 'ኣብ', '241', 'ቀዳማይ', 'ብርኪ', 'ኣብያተ', 'ትምህርቲ', 'ወረዳታት', 'ራያ', 'ዓዘቦ', 'ጋንታ', 'ኣፈሹም', 'ታሕታይ', 'ማይጨውን', 'ወልቃይትን', 'ክትግበር', 'ፀኒሑ', '።'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O']}\n",
      "First test example: {'tokens': ['ተራ', 'ዜጋታት', 'ምበር', 'ላዕለዎት', 'መራሕቲ', 'ኣይኮኑን', 'ዝጉድኡ', '።'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(\"First train example:\", dataset_dict[\"train\"][0])\n",
    "print(\"First validation example:\", dataset_dict[\"validation\"][0])\n",
    "print(\"First test example:\", dataset_dict[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "5c3e64a3bf9245e784ebd43e0d781303",
      "5e1bca9e2b07414194cd780fde76ef98",
      "00c0de91e336444899fb50f2819fc614",
      "d16c0a3efa7b4c899c7f003152e20ace",
      "c5902bcf81a3451caf9abc86352e46eb",
      "b06dae15ae6e4998b3fbd841da294104",
      "c10f9e7914524e9f9d51d1353cc0c721",
      "25655bf1a3dc46789c0b2ea4d54a76bd",
      "f53e07f5c808401ea8d5d8b86aaadfb2",
      "5a5d5ddd839c462abdd84795e36dd2f7",
      "8078e234ec954854b3b7f6d28707a062",
      "1ec1e4930d15475f99c0d64e20e967eb",
      "3f97ce5c8f1d4a4d802678c6bffc21d9",
      "a04b29c2c9144c2ba388041b113cc2b9",
      "d8bd8a917ddc43b4b80566d6f1adf32d",
      "9ac127e6195f47e98bca9261b8ac7a99",
      "65323527a75f4a06960c74d4e3fafcc4",
      "37d7da818ef04ca1904beeff7681bf27",
      "564bf032560a469fb6b25206457399fb",
      "25125c41a9ca4b7fbf6891a957e97c28",
      "3631229938da4e5590ea03dd8b18b1af",
      "ea079ec7b4474b71875f548842a0a621",
      "e76a5f5eef324b9fb2ae89217127b006",
      "0e6c7f5da12c462fa19383ce6fa52754",
      "4fa909bfede840bd9eda105e10b1d263",
      "f3b8f94ac1af4f95b178efd49f5797be",
      "51e69583e44442818d7c7d228376f040",
      "61416c66396a40c8a65d68571ab7b908",
      "32fe75d9117144f9bc68f0c78ae7e186",
      "ef18d61b54f049cab72aad9cc30337c8",
      "6c150132814349249472ac8ac7ab8657",
      "29c73ec593f34defa854a5a97d3b5802",
      "353d49a6f6c74366a1f956f3c73e59d6",
      "73c1f039a2fb47c781aa2913ce543ee3",
      "557eba6d343841079dabe36d1a999e18",
      "07c406fd183e4fcc8ef12052cff2133e",
      "e5b79936090540f6bdd787c162ad8a00",
      "a73f3a2a5a384e02b1b8d0f8e64aa5fc",
      "4d7bf5ec3d204a6ea3183083eac2e03d",
      "ca1aca1238a0478bad086536f2373c5b",
      "92a14173717a42f3acf4770cede6aa11",
      "0d4aef2a2d1e43dc9df57fbe60d9ea5f",
      "59cdbae2cda74038bf1e8c01bb1fc5a2",
      "c0a5f2cc19dd4fe39babff1000d74c76"
     ]
    },
    "id": "c7WgZXmm687k",
    "outputId": "43001c49-8eb3-479e-bed2-d60c47b15751"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e64a3bf9245e784ebd43e0d781303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec1e4930d15475f99c0d64e20e967eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76a5f5eef324b9fb2ae89217127b006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c1f039a2fb47c781aa2913ce543ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"castorini/afriberta_base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_base\")\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnY64wZf7RS_"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # Convert NER tags to IDs using label2id\n",
    "    examples[\"ner_tags_ids\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n",
    "\n",
    "    # Tokenize the inputs, apply truncation and padding\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", max_length=128, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags_ids\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # Special tokens are ignored in the loss calculation\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore sub-token labels\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCGySzUa7Wod"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=2)  # Get the predicted labels\n",
    "\n",
    "    # Remove ignored index (-100)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "a6a3386980d9478cb9b0a64ac76bf53f",
      "f531d93a8c824c26b2766845531d2c7a",
      "8a5c2055012d43cebf8c44707f37697d",
      "14ed1686e98b49358aa2a6526bf888f1",
      "2f180cd9b3ab44109b0efe54a200a47f",
      "01e9eb05cdb04b9d88a40a32db13dbc4",
      "9b176e803adf4d759a908262a30c086b",
      "4de6a32d246045f6bdf59d068afe750f",
      "1b448da8205e4d3c8e6f9862f6c5bca4",
      "bc48599dfe0946dfaad138fd278f7cf1",
      "9952b45d28674897b72ec4b36d78b8d5",
      "47a6874611764c53b0cda08cfa84f705",
      "e5750cb2feae4184a6505b53978976a0",
      "7f09f3270eab4301a22ad3876dc6a4b0",
      "cda219510a0f41dd94d9e7031b1aeb23",
      "3a6b13e3dd724642b4a925ae4349796a",
      "140a19bddd3b44f99c37d21cbe1de40d",
      "d85ba4cdd43f4806976758e4f40d1244",
      "238fea652730413fad8e9aa1984a3c1d",
      "5732faf53d3f4c23b8aa23bdee139f7e",
      "72c2b90fdcc4413d969ed0ea4bed8589",
      "edf8218d49054f6c95839f3af7f3471a",
      "458e77f698224a1ab6d77dea4afea909",
      "430a27c2004d4c81b26167f32fa6b79f",
      "51556527b8eb47c2b916c92c13c4aa5a",
      "5414219167874a398c88191c0985a224",
      "2ce19d6607e346178ce78c606960fc3f",
      "63a933d1230b457585f9a7714347d9fc",
      "42443e3adecd4e608ae2be408bec74d1",
      "1bec255621b9494c8357c02e82bddb18",
      "ef178c1382e045159200034d4d69ba7b",
      "a896ac48c147485692cac2245a006ac5",
      "73a56c8587444d5ebd64ccafbce96ebe"
     ]
    },
    "id": "QFhlCnUW7asw",
    "outputId": "be4bbb0a-9acc-4d60-d986-f0c167cf13b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a3386980d9478cb9b0a64ac76bf53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a6874611764c53b0cda08cfa84f705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458e77f698224a1ab6d77dea4afea909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\", \"ner_tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "0a46e77aedbf4af9843ae2b00a77f031",
      "6b70cf431a8047d0846eaac8990eb098",
      "89ed8836a1fd4329b048163945dc0deb",
      "1d0f9db59b684eb3a504f995dbdfb289",
      "c2635ef0cf604cd781a20292899c844f",
      "1837dbee02b54797a5298a07de051a0e",
      "6b19a20aaa5a425fb09d2df0b59a7d4a",
      "8e622f1347b14f61ac83eb4286b6722c",
      "9d805dd92f504c20b1b532b8ce0d291a",
      "e78e09dbcb674a1e8c18053134ec7179",
      "ee3a5e33909745aea1c4634d44488ed6",
      "8a8dd58fc321435586fcfc3f15e50e67",
      "1d8261d660624bf9bffb1dd1891a312c",
      "e0a68c6b60b24b9481d35a44452a81b8",
      "7924de6bc37448e4980411bae594fce8",
      "25e6f658a4174eed8cdee1df14029689",
      "dc1f9417ff8d44a4905aaf448a7320e0",
      "de5ecb4cd2464a589f14936497f4894e",
      "e829f8a5f9d14e64b21aeb9b68b5ccfd",
      "d430e54aa780429b819926db28da11cb"
     ]
    },
    "id": "Sy2HEdonlwZk",
    "outputId": "86d48a78-4991-4efe-c95b-d2fa1d003d38"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a46e77aedbf4af9843ae2b00a77f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pobm9G8L7hWc"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/Tigrigna-NER/afri-berta-tigrigna\",               # Directory to save the model\n",
    "    eval_strategy=\"epoch\",         # Evaluate after each epoch\n",
    "    learning_rate=2e-5,                  # Learning rate\n",
    "    per_device_train_batch_size=16,      # Batch size for training\n",
    "    per_device_eval_batch_size=16,       # Batch size for evaluation\n",
    "    num_train_epochs=5,                  # Number of training epochs\n",
    "    weight_decay=0.01,                   # Weight decay\n",
    "    logging_dir=\"./logs\",                # Directory for logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,         # Load the best model after training\n",
    "    metric_for_best_model=\"f1\",         # Metric to track best model\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQEdQ1Qt73yR",
    "outputId": "b6c2bebc-095d-4182-c4d0-4545a95b8fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at castorini/afriberta_base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "ennyKY1s77FL",
    "outputId": "0161b1e7-4cfc-4333-d286-d9bb6c0efef9"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-33-1677865715.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250621_175231-5j3torie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/5j3torie' target=\"_blank\">/content/drive/MyDrive/Tigrigna-NER/afri-berta-tigrigna</a></strong> to <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/5j3torie' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/5j3torie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1403' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1403/1430 06:33 < 00:07, 3.56 it/s, Epoch 4.90/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.183134</td>\n",
       "      <td>0.655819</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.658742</td>\n",
       "      <td>0.940848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.167072</td>\n",
       "      <td>0.665133</td>\n",
       "      <td>0.719403</td>\n",
       "      <td>0.691205</td>\n",
       "      <td>0.944505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.160442</td>\n",
       "      <td>0.713307</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>0.719290</td>\n",
       "      <td>0.950722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.167918</td>\n",
       "      <td>0.704630</td>\n",
       "      <td>0.757214</td>\n",
       "      <td>0.729976</td>\n",
       "      <td>0.951362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1430' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1430/1430 06:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.183134</td>\n",
       "      <td>0.655819</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.658742</td>\n",
       "      <td>0.940848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.167072</td>\n",
       "      <td>0.665133</td>\n",
       "      <td>0.719403</td>\n",
       "      <td>0.691205</td>\n",
       "      <td>0.944505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.160442</td>\n",
       "      <td>0.713307</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>0.719290</td>\n",
       "      <td>0.950722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.167918</td>\n",
       "      <td>0.704630</td>\n",
       "      <td>0.757214</td>\n",
       "      <td>0.729976</td>\n",
       "      <td>0.951362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.168130</td>\n",
       "      <td>0.718456</td>\n",
       "      <td>0.759204</td>\n",
       "      <td>0.738268</td>\n",
       "      <td>0.952825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1430, training_loss=0.14626189945461032, metrics={'train_runtime': 415.826, 'train_samples_per_second': 54.855, 'train_steps_per_second': 3.439, 'total_flos': 993502363015680.0, 'train_loss': 0.14626189945461032, 'epoch': 5.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "aqlF-odIoCa9",
    "outputId": "87308910-405c-45ae-d744-622b997225c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16543681919574738, 'eval_precision': 0.725912067352666, 'eval_recall': 0.7585532746823069, 'eval_f1': 0.7418738049713193, 'eval_accuracy': 0.9538298248717495, 'eval_runtime': 3.1823, 'eval_samples_per_second': 179.429, 'eval_steps_per_second': 11.313, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "1f673a33c09e4b8da2bc9d21f59c9abd",
      "f8b8668163ec44b291c963363592dfd3",
      "6ddc8bb2e5d649a99d6daf85c437599a",
      "f74d51002ec845eab68f7dd94a6a0519",
      "ff3fbc2c828b426b925abd3bea210e99",
      "d503b8bd3b904d63b1ca5c8af436f227",
      "0ccb046aa3d34b2e98a560d9565d8523",
      "fee2d4a05af94bed94f3729605ec5b99",
      "1d2adbbd3af24871bd77166daa69e848",
      "557f39077d0f4608b7878b3f63d9a625",
      "67b6e016cf994d098479c84f8c72d063",
      "3241eef1b6e44e5193d94fa0c87d7b36",
      "f5aa159557ca48188b15894b91595766",
      "847f323016f34e9ab3e4d9b618e3d2e1",
      "093bf433252841e9b1d9df2ec7492298",
      "587ee10dc8504286a8bf9822701aa589",
      "bfc87cc8defb4884854cb30bbfb9cc03",
      "980f4aa9b2844df8b5dc088e098517e9",
      "4b9be585b2be450a9725f5a93fb5f9ca",
      "f0a2a6d1ba4c4f859ca460868416dff6",
      "bc5c09dea2054972af16794a1d54524a",
      "9f587d683f1d4bc4bf5df354f144f711",
      "369d350faa384f7281330e44f2081087",
      "4f8a0a485efc43aaba1b7a4bca8555f4",
      "6b2e1abbb4c54b31a34fec052ef553f1",
      "562f2d0e6a654ffc9d3e2e9be500dfef",
      "1e489c5cd93142e2b478316348c38038",
      "67ed067dd40d4f0a9a583e06f661284d",
      "ad7e4358838b456fac993455497aa63a",
      "d5c21695e61a4941a4fd4007f25bb45c",
      "f52b402ad3ec49d3a5519507b8dbf1d9",
      "ecec95feb93a46e3ac69c373a4d72c88",
      "dead6913e3044549b68628ef83ea1a3a",
      "6ab58d97b57e42aab4918abec9c4449a",
      "442b83987ed14d778239d21029e8a02d",
      "c9f8d3dc12264e929af7052c23fb6ff9",
      "0fd0be59ea4d4263990d63e792461605",
      "41e71a39a7a642a598eb76518c5588e7",
      "9509deb6b59a40c5961037add6f0d484",
      "53208021615643458e3890e9a3bbe071",
      "8187a86b82ec43be90a074cd1de0a301",
      "4b03cb07c3f844a4a437bfc9b0ca3bed",
      "2108bb85bde7446094223fc8f8a7f229",
      "c4416d37f4f84f48ad7a156acdbe37a3"
     ]
    },
    "id": "DtyrayfuoDHO",
    "outputId": "584b04b9-adef-4035-9902-3165761d81e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f673a33c09e4b8da2bc9d21f59c9abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3241eef1b6e44e5193d94fa0c87d7b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369d350faa384f7281330e44f2081087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab58d97b57e42aab4918abec9c4449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/444M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Elu-dan/afri-berta-tigrigna/commit/1d977757ce3b2336e93cf6fcda334be87e1fa577', commit_message='Training complete', commit_description='', oid='1d977757ce3b2336e93cf6fcda334be87e1fa577', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Elu-dan/afri-berta-tigrigna', endpoint='https://huggingface.co', repo_type='model', repo_id='Elu-dan/afri-berta-tigrigna'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "714edb35f64c4afc98b3567ed7b50fe0",
      "ff52da4ec46d485ebd9b7ba575755c15",
      "8832769e91d84dfb9bb33d99db0a30a3",
      "27a2ec0d767a456e86d1060da1f48485",
      "c27678abf60b4d40a2b311aeeaf3eec9",
      "b6a4a2f7b0504966962827127c2cf3a3",
      "f2508b2fd70848fb9ad258c303750be7",
      "4d635f7129b14845b095d8c73c4fa186",
      "2c1388facb344d9bbc7d7bc317723284",
      "cea1d7d2d11c43dab7775477ed8c2d42",
      "4133ef97ffe243f5b61006b4b743eb90",
      "5fce7040ae324792adf539283e858105",
      "38164681ad1c488f915788c4d950e845",
      "8e0944920eb84dd09354453f45364644",
      "e0971cc3be57469aa675620293f642f0",
      "9c0b5b52d0b340feac15016aef29f255",
      "818ae2f985784f97bc7f59879d84e780",
      "5ee3287936f448909d7e93b684af03fd",
      "21aeab5545de4511b680ef0e2a4868f2",
      "47a8e07c94a64451a3d8e5f799f1a47e",
      "3f57b09cd93943608d472ca6299efad2",
      "09adb1caf91143be8e6991b6f5c8754d",
      "81021a8e95d3472394971cd69509b04a",
      "231fa625f4de4e758d40383216c25917",
      "1106bfc4e9914217b6ebf4ca3fa159f5",
      "45a02f838b9c490aa74d68c631a26c5f",
      "b1797f7e08904d67913e1a36df82d849",
      "79366b840efe401496866d7c1b6cdb53",
      "ff8c4acfa53f4d29b0b4e04f240ae39d",
      "13a756cac0bf4aa5a7c1bb1406a729ab",
      "e88516ff1e4a406192380f4f7abd888f",
      "05686371472340528b68918b8e751823",
      "61d7ecac277244e4b4c18fb35340db51",
      "713a90dd9cd14a6782d4662067f8ddcd",
      "d39316d7a1cc4468a89b42da7abb3385",
      "55bc8f42fc3746c6bbd9cedb47385b1e",
      "1989aa42681642eda11baa6be75b8b8d",
      "f2e67c9eca694baa81dfe489ed2b874a",
      "0a8f107e3544468693f698ec7edaa469",
      "2a1f7575c7ff42dcbab5ae8a730db61b",
      "19ba2873a5c04d9ba0ac5e766685d277",
      "1ffc4f10e2bd48fb88d78a603a7ed612",
      "e048a8ee9daf4657b8850287ee378833",
      "017a6b892e9d4373a486951e05d1ae03",
      "9e812882566245f4b27515b215b594e3",
      "3d653fd098a84ab2a3ea1cbfa9c67bf3",
      "7cc8f90ff2154a30ae4f47b6d33c778f",
      "788d3bdd41b54e39bbb88be41a3cdf81",
      "f950d8f4667e41b2a3f65eaedee1c373",
      "0c315b44d5ab403e9b4a691aea9cd697",
      "e99e61de195348ddbe0bb060d886f542",
      "d9700c0c24824a98a41a381b86c6c732",
      "e3f9983a5423414789cdd14484e92cac",
      "c672a919a7f84e00b10ea7b5d8a3058b",
      "3fa39ad5bd3e40859c4708a664ea0529",
      "2be876e9fcf3420d9652367b60e89a45",
      "7b2f5acba191450692c1fd20ff45a815",
      "f14f7d088e2a47a0bfade04a625693c5",
      "24edb59058da487eb343e883fbf35a6d",
      "68b1d530ae35430a8db390bdeb842605",
      "2db984ea1726422fb9fa01671b05a411",
      "f34ebb0719864cc1823799e13e213fb9",
      "1e3a234501a44b13ba69fa2da1f4e14f",
      "68187cf342ea4385b5080ba63c281344",
      "1c6d36b374eb492cac7c1356baff14a5",
      "d6ec195648414d1baef023f3f78554ab"
     ]
    },
    "id": "pzYZXSTsqyEZ",
    "outputId": "4576dacf-2ee9-4e29-dbc0-f38dbb5726f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714edb35f64c4afc98b3567ed7b50fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce7040ae324792adf539283e858105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/444M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81021a8e95d3472394971cd69509b04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713a90dd9cd14a6782d4662067f8ddcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e812882566245f4b27515b215b594e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be876e9fcf3420d9652367b60e89a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.94669515),\n",
       "  'word': 'ኣንድሬ ቡሉዋ',\n",
       "  'start': 0,\n",
       "  'end': 8}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Elu-dan/afri-berta-tigrigna\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"ኣንድሬ ቡሉዋ፡ ኣፍሪቃ ዝረስዓታ ጅግና ተቓላሲት ናጽነት::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bixH8vmGKMcA",
    "outputId": "e3b45eaa-ceb5-452a-9db5-0fc53a85f9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': np.float32(0.9633071), 'word': 'ኣብይ ኣሕመድ፡', 'start': 10, 'end': 20}, {'entity_group': 'DATE', 'score': np.float32(0.8109887), 'word': 'ድሕሪ ሒደት መዓልታት', 'start': 35, 'end': 49}, {'entity_group': 'LOC', 'score': np.float32(0.9808646), 'word': 'ካይሮ', 'start': 52, 'end': 56}]\n"
     ]
    }
   ],
   "source": [
    "result1 = token_classifier(\"ቀዳማይ ሚኒስተር ኣብይ ኣሕመድ፡ ናብ ስልጣን ምስደየበ፡ ድሕሪ ሒደት መዓልታት ናብ ካይሮ’ዩ ኣምሪሑ።\")\n",
    "\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnv98g8oK-UN"
   },
   "source": [
    "# **HornMT Dataset**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUn8cmk_LCem"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/Tigrigna-NER/tir.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vglRPRusLx9X",
    "outputId": "cc725506-e8eb-4793-c10c-6e7203363767"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2030 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing: 100%|██████████| 2030/2030 [00:25<00:00, 78.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQZesgMVL4Pr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_hornmt_tigrigna.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vswC8jHwL5Kj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_hornmt_tigrigna.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n9IzFrLxaPX",
    "outputId": "ee510539-5559-43da-f7a1-b02ebc5fd7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/hornmt_ner_results_tigrigna.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwtuuZiOsj2e",
    "outputId": "01eefc53-a5ef-4036-eb1c-064439d6f7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 1325 'PER' entities. Saved to /content/drive/MyDrive/hornmt_per_entities_tigrigna.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_hornmt_tigrigna.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/hornmt_per_entities_tigrigna.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfo3HsgSMEMh"
   },
   "source": [
    "# **Flores 1 Dataset**\n",
    "\n",
    "https://github.com/facebookresearch/flores/tree/main/flores200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYUWuadqVHHz",
    "outputId": "d343b72d-8511-4f7e-90e5-ddf48b008125"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afri-berta-tigrigna\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDqOTWYrQAjz"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/Tigrigna-NER/tir_Ethi.dev\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqoxbZwdQWk_",
    "outputId": "d1f2bc4b-5a9f-4d30-b6ab-4445076edff7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 1/997 [00:00<05:57,  2.78it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 997/997 [00:09<00:00, 102.05it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at8MJPahMDvW"
   },
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores1.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmhvBsZjOgSG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores1.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bic9aLP5UReu",
    "outputId": "ca31cde2-0dd8-445b-f6ed-1ecb2528b0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores1_ner_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARkbQwBpU0Yc",
    "outputId": "8a0e9d2a-1ec7-4e94-8d24-017a15aa793b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 297 'PER' entities. Saved to /content/drive/MyDrive/flores1_per_entities_tigrigna.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_flores1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores1_per_entities_tigrigna.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i_S0m-GvrO8"
   },
   "source": [
    "# **Flores 2 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byOSIzgOvquI",
    "outputId": "6c4a7bfe-e560-4172-ba77-e8b29c69f6e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afri-berta-tigrigna\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epAXUDoqv_aY"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/Tigrigna-NER/tir_Ethi.devtest\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV5WaVQ9wCzk",
    "outputId": "42e4cb74-d2c2-4026-8df4-1adb86330a2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 1/1012 [00:00<03:50,  4.39it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 1012/1012 [00:13<00:00, 75.55it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1chESCDjwD0G"
   },
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores2.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiWn3fFDwH7q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores2.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSJC6YjqwLMg",
    "outputId": "c9bc0dc2-b347-4bc8-bbf4-a730f22770bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores2_ner_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxZvOCMXwO1n",
    "outputId": "1a506eeb-6eaa-4c01-d322-2fb11515a563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 308 'PER' entities. Saved to /content/drive/MyDrive/flores2_per_entities_tigrigna.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_flores2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores2_per_entities_tigrigna.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlBVISyWb-2O"
   },
   "source": [
    "# **NLLB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEB_x5i655nq",
    "outputId": "5d8c028f-703d-4b36-834a-352e326fbfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into 20 files in '/content/drive/MyDrive/split_files'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_text_dataset(input_file, output_dir, num_files=20):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines per file\n",
    "    total_lines = len(lines)\n",
    "    lines_per_file = total_lines // num_files\n",
    "    remainder = total_lines % num_files\n",
    "\n",
    "    # Split the lines into chunks\n",
    "    start = 0\n",
    "    for i in range(num_files):\n",
    "        end = start + lines_per_file + (1 if i < remainder else 0)\n",
    "        chunk = lines[start:end]\n",
    "\n",
    "        # Write the chunk to a new file\n",
    "        output_file = os.path.join(output_dir, f'output_{i+1}.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            out_file.writelines(chunk)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    print(f\"Dataset split into {num_files} files in '{output_dir}'.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = '/content/drive/MyDrive/ti.txt'  # Path to your input text file\n",
    "output_dir = '/content/drive/MyDrive/split_files'  # Directory to save the split files\n",
    "split_text_dataset(input_file, output_dir, num_files=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct_2nJsYygN0",
    "outputId": "cc3df28c-cf41-424d-c412-f689968a15b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afri-berta-tigrigna\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikQ_Ut8q70iq",
    "outputId": "02d94d84-56f3-46b1-98cf-76a3ac694555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 8/70982 [00:00<40:30, 29.20it/s]  You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 70982/70982 [09:57<00:00, 118.85it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/content/drive/MyDrive/Tigrigna-NER/output_20.txt\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqz22ukt8rw4"
   },
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/output_20.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuZWJIO8kVIN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/output_20.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-fx5_PjkVFV",
    "outputId": "9ab731e4-d6ed-4daa-853b-90248ac32a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/nllb_output20.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uax29RYkkVCO",
    "outputId": "0230d1e7-450d-45e1-87df-5dbbd3a3db54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 19128 'PER' entities. Saved to /content/drive/MyDrive/nllb_output20_per.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/output_20.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/nllb_output_20_per.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INmYFlWUkU_F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9zwd4BIkU8O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcAf7MKbkU5W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKRUQbZykU2g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW1ivjNjkUu9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNHVmHMAa-iS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
