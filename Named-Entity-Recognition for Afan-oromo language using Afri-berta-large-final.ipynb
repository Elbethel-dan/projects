{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-jtKWSFIlwK",
    "outputId": "b4fdeb85-6867-4ad2-b653-6b6c15239a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkMFZyWYv-9f",
    "outputId": "0485fd75-0b8d-4e07-fdc7-4d211f18ded6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=0.6.2->dataset) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=0.6.2->dataset) (4.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.2.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=02fd4ce222f25043151e7bd62d61a8cb1ebd25a1c4adaa9b3e6810f5e1d79b2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: banal, sqlalchemy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, seqeval, nvidia-cusolver-cu12, dataset\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.41\n",
      "    Uninstalling SQLAlchemy-2.0.41:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.41\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed alembic-1.16.2 banal-1.0.6 dataset-1.6.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 seqeval-1.2.2 sqlalchemy-1.4.54\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate seqeval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDX4-JZrwTb2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvG8jJWnwVmH",
    "outputId": "f21011d6-cd57-40f5-c924-21c0e1c498b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence: ['Af', \"yaa'iin\", 'Caffee', 'Oromiyaa', 'aadde', 'Loomii', 'Badhoo', 'raawwii', 'hojii', 'gurguddoo', 'Caffeen', 'hojjechaa', 'ture', 'ilaalchisee', 'miidiyaaleef', 'ibsa', 'laatan', '.']\n",
      "First Tags: ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def load_conll_format(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(ner_tags)\n",
    "                    tokens = []\n",
    "                    ner_tags = []\n",
    "            else:\n",
    "                token, tag = line.split()\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(tag)\n",
    "\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(ner_tags)\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Afan-oromo-ner/dataset-Afan-Oromo-new-merge-one.txt\"  # Replace with your file's path\n",
    "sentences, ner_tags = load_conll_format(file_path)\n",
    "\n",
    "\n",
    "print(\"First Sentence:\", sentences[0])\n",
    "print(\"First Tags:\", ner_tags[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDciSwC-wdrR",
    "outputId": "1b79d216-cf53-4702-a5ce-646c1e0ff58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Af', \"yaa'iin\", 'Caffee', 'Oromiyaa', 'aadde', 'Loomii', 'Badhoo', 'raawwii', 'hojii', 'gurguddoo', 'Caffeen', 'hojjechaa', 'ture', 'ilaalchisee', 'miidiyaaleef', 'ibsa', 'laatan', '.'], 'ner_tags': ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(sentences, ner_tags)]\n",
    "\n",
    "\n",
    "hf_dataset = Dataset.from_list(data)\n",
    "\n",
    "\n",
    "print(hf_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "c9a8426065074fccb75b93a698f3a9f4",
      "a06b24012bd347df839dc1587c22c242",
      "38e8bd91c5044fd59142d60653ecfad9",
      "abcf193efd9d438f90e920745d882326",
      "e0b21e8b18e24183b6279045883c8b5b",
      "c77cafb0a6324572b452cdb376765274",
      "99559a6776864f1ab2f294bcbd251b6d",
      "a670c8031c9444daa7478d9bb90f5b31",
      "b466c65c6f0a408182721793acb3d710",
      "b558c61a9cd7425abf1e6e45ce89e732",
      "e014ca46131e4055badd40d572c1700f"
     ]
    },
    "id": "tXyVyLH0weiP",
    "outputId": "0f4b3b27-88fa-4817-97c4-fcff6840708a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a8426065074fccb75b93a698f3a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Af', \"yaa'iin\", 'Caffee', 'Oromiyaa', 'aadde', 'Loomii', 'Badhoo', 'raawwii', 'hojii', 'gurguddoo', 'Caffeen', 'hojjechaa', 'ture', 'ilaalchisee', 'miidiyaaleef', 'ibsa', 'laatan', '.'], 'ner_tags': ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner_tags_ids': [0, 0, 5, 6, 0, 3, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Define label-to-ID mapping (update with actual tags in your dataset)\n",
    "label2id = {\"O\": 0, \"B-LOC\": 1, \"I-LOC\": 2, \"B-PER\": 3, \"I-PER\": 4, \"B-ORG\": 5, \"I-ORG\": 6, \"B-DATE\":7, \"I-DATE\":8, \"B-NUM\":9, \"I-NUM\":10, \"B-MONEY\":11, \"I-MONEY\":12, \"B-TIME\":13}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "\n",
    "hf_dataset = hf_dataset.map(lambda x: {\"ner_tags_ids\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "\n",
    "\n",
    "print(hf_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPZKlbLmxoo6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_sentences, temp_sentences, train_tags, temp_tags = train_test_split(\n",
    "    sentences, ner_tags, test_size=0.2, random_state=42\n",
    ")\n",
    "val_sentences, test_sentences, val_tags, test_tags = train_test_split(\n",
    "    temp_sentences, temp_tags, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(train_sentences, train_tags)]\n",
    "val_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(val_sentences, val_tags)]\n",
    "test_data = [{\"tokens\": tokens, \"ner_tags\": tags} for tokens, tags in zip(test_sentences, test_tags)]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0_qWb_-xur5",
    "outputId": "d5dbb9f9-c06f-4566-f4e5-21b8f72aeb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 1205\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 151\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 151\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "74aa4d2af46641cb8608b32b8160645a",
      "c070e1c294b44ae691291989f5fd4af6",
      "e99f733172864f68a2479075e315e12f",
      "4d9f157d65074637a18e681aa6c4265d",
      "5fc7bafa78404776a71f231247b87395",
      "d882b597f426482aa78470f38860bd68",
      "cd96e470dee747e08fb11d5c7f34be17",
      "8e44ce642fb145058e7c98d64a3cadbc",
      "dc1bf866d3734eaa9ce80909edf624c5",
      "1951632e66944787a2b37485d1c5592b",
      "c90cf0325c1b4f288c785a42178f6a48",
      "a1ca77a2e56b4d5594f55f6dc22b70e2",
      "0aba2b819c3b472ba59bbd8a90f6bd82",
      "e80ee80549cb41bea3438992ee808e30",
      "7d3ec8c622fb4bdfa8e9e947aed4ead9",
      "3deae5265b084810829ffc9af834bc4c",
      "f85db76a1a8c429bbc0dff55dc8a3029",
      "d601d20aa8494754a2023f1a057d2165",
      "54ab78e6dd2442158318a4b7b9dea7df",
      "272dfc96e12b4068b9e50061e5cbac65",
      "455aeb69cc8943fe8fbe22380b38e340",
      "9b68e6f2da2b4996adc44ef40782d15b",
      "3257a69704394dc48bd86c92fc03b57c",
      "5453fc4ecaa14a0686558909953ed0d8",
      "a018956439914265987ee471a47888b9",
      "3a3834f0716242d8b24bca1920b28b84",
      "7efce2f349b845f0a0e2eb6c41f74278",
      "a663eaa9055f4e94b26d2d5e1aaa8d8e",
      "388acbdb8b9e44698219c9a650c1fe59",
      "00450daf95bd4b7695bbb704529d164b",
      "dc30d586a80b43e9888fab557c0ab217",
      "85042c3903b340b381620c3ef18cdd63",
      "aefab06025f4476e9c0b37095efbbb27"
     ]
    },
    "id": "2eTRHgxBx0KK",
    "outputId": "8317131e-bd53-482d-f2e6-4a269200f5ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aa4d2af46641cb8608b32b8160645a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ca77a2e56b4d5594f55f6dc22b70e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3257a69704394dc48bd86c92fc03b57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict.save_to_disk(\"/content/drive/MyDrive/Afan-oromo-ner/new-ao-dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGPkZ2DUwkWF"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"validation\"]\n",
    "test_dataset = dataset_dict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffwGKoKE4fgH",
    "outputId": "f0e54b8a-603e-403b-9c5a-7dddcdae9657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train example: {'tokens': ['Boordiin', 'Filannoo', 'Biyyaalessaa', 'Ityoophiyaa', 'filannoo', 'marsaa', '6ffaa', 'biyyaalessaan', 'walqabatee', 'jijjiirrama', 'teessoo', 'fi', 'naannoo', 'filannoo', 'naannolee', 'fi', 'bulchiinsa', 'magaalotaa', 'kan', 'hin', 'fudhanne', 'ta’uu', 'beeksise', '.'], 'ner_tags': ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-NUM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "First validation example: {'tokens': ['Masaraa', 'mootummaa', 'keessaa', 'Mallasiin', 'awwaalleen', 'baha', 'jedhee', 'yaadee', 'hin', 'beeku', '.'], 'ner_tags': ['O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "First test example: {'tokens': ['bara', '2007', 'keessa', 'hiriirri', 'bahamee', 'ture', '.'], 'ner_tags': ['B-DATE', 'B-DATE', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(\"First train example:\", dataset_dict[\"train\"][0])\n",
    "print(\"First validation example:\", dataset_dict[\"validation\"][0])\n",
    "print(\"First test example:\", dataset_dict[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "586e8f9582e54f9682f0a805e3e97165",
      "0c7db1d2d57240abb96a8df4a001160b",
      "e905b1045bc2497d9d5bb32253e7ae54",
      "8fef79a219ea4ed587f2ce12e19aae3d",
      "aca5941d203148219d069caccfbc542d",
      "e9a09914914a42f1ad37eefd8a982ec6",
      "992e7a282f2d49f98104ff7e3ce58b47",
      "2438982d046e4533a5cdbe737a627e72",
      "2e8fe7ff572e4c758d134d5f8dc63004",
      "854fd36dc2004cbe95a676cd262c524c",
      "5c6a576a23594a75b789fb3a69cbaa23",
      "d55229a862bb47d29c3b2ac47a137a56",
      "66098cba887646bb8977be74a4b1236f",
      "fc5497d57653456f84f964f2b9df8c17",
      "8697be0ba1fb40ebacf4ec6faa9d70c9",
      "41b8ac6ac78240dda3e5ca89d304030c",
      "a87b2cd3670e40fb8ebf6f52393fe28c",
      "61644bfbdb8044979381a58bc6af1ba6",
      "db1986d1147c441680cbde568bd4801c",
      "7ead432f75754bc995e96b92fbd3c3c7",
      "0d7592ea01364ea7898bfe65d9b28b80",
      "2e37d4656aa24d3f814422bc32447cfa",
      "9a7385e103c04679b830284fd37be94b",
      "abfdc68b9d094db6b43802fda127c9c4",
      "bbede7e43f964da88b7988a9dfbba669",
      "7febbbcc753e4d1ea0b23a0677d4557f",
      "b69f39d7471a45f7a64bd40c964edf8f",
      "1f4a56e7671e44a4a19e0acce3cdc284",
      "cd080d3a67604b5485a215cef4eb5566",
      "7df6ff4c9f1d4b318abda5a2022e26de",
      "a99814cc29cc4acfa014c6e90f17d6b8",
      "db0187f0524d4e29ad8f783edeb2b2ae",
      "4a2ed6b58cff47cbbb0bdd5942daba6a",
      "9448ccf50ba341e3ad793621da4870a8",
      "9ad3ff9aa56744aeae08d3f38d386d71",
      "da5a30299ddc4c9c8954ad5fca2e6ff3",
      "7741560d842d463297c1d3368b223b3e",
      "25397ac6f7014af4879b800ecb40391c",
      "d85e03bcc5714e3289266fcaad51de92",
      "dfcd06d662c9446c85d6f2c8f09a9ad0",
      "c26bc87644db402e9ebd70f720ae8109",
      "3692e727814d41838b06e12263038d3e",
      "264e63e4680d45738c9139a625725d96",
      "06b68f93c83f43fbacd6d10ef7574d1d"
     ]
    },
    "id": "LOTU0nbZwoaS",
    "outputId": "a2d38406-0172-40cc-ad3c-7a63d3dcdd5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586e8f9582e54f9682f0a805e3e97165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55229a862bb47d29c3b2ac47a137a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7385e103c04679b830284fd37be94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448ccf50ba341e3ad793621da4870a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n",
    "model_name = \"castorini/afriberta_large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD1Y5PGpwvz9"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    examples[\"ner_tags_ids\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n",
    "\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", max_length=128, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags_ids\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbzpRgClw0IP"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "67299d7b9d4841c1964df59f43171e72",
      "fe91d988ed464a32a582c560e6b2b18f",
      "472685ffcd114bad836b425eb29e4439",
      "5965c3d67e684a4f8e5204d17940ef1a",
      "fb3650442ec34c22b673ca2de7bbf405",
      "da3459ae8d29499185131e1e1055b640",
      "d956492efdfc471a89ce328bdfcddd39",
      "70f8d775f94b45c382ac44a020c3368b",
      "d76c7129ecf94d9497225611e9c86aa7",
      "b6e8bb8fc6c4498287d674894ad4640f",
      "f83b40797e2e4b60b2153be0c73ad77e",
      "6242cc03a2ee4096872fe6909e8a0409",
      "483d526313074c38b4d0527330534e6d",
      "543e1e30abca42cf98e1c8b7992fdbab",
      "6d6fcbaa5a094d76ae9a043bfc71bd19",
      "64ae5e3301ff457a92be7d1706357b1c",
      "568cfc32f6ac45b28c5286496c8dc376",
      "8eddc96439344ce9afb25ef23dc53538",
      "3dc022a2d8c34a80ab5af7fc3ec0394a",
      "08f941bb43834148acb59987cc821119",
      "be98b13a264f4e28bfca4c8a062585f4",
      "1b0d671d7eab414f988da5c4ae6ea95c",
      "209f069ed05d45168ec9eb0e21385ea0",
      "f350a82834ad45d2b09340808789a813",
      "1ebb638d974b42dca37729e99309376e",
      "473562d13a4048788fea1415fe69b341",
      "63a0709a0e454b5e8b519dd51d535108",
      "b099a230dba04f1ab347874457e8662d",
      "5531b4a720e145d1aa767c1f4dddb6da",
      "704d5e446e0145f795c7e0275f0dbbea",
      "baab6e3165814934904e729054bf3638",
      "8f0a08ff3cd54b04af79159daed5ae27",
      "54572a551c764c23819c3201005d397f"
     ]
    },
    "id": "a10BF7sRw4Xm",
    "outputId": "5b7d8c2b-6ae9-4601-e3cf-d54368b35448"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67299d7b9d4841c1964df59f43171e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6242cc03a2ee4096872fe6909e8a0409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209f069ed05d45168ec9eb0e21385ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"tokens\", \"ner_tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "918e2c14129747819503ec84827368c5",
      "7399de8db90746719caa8dfb6642661c",
      "566cdf19add8473f9bf544023199fe0d",
      "aca5b9b5e15541449dd7e4ea7f4cace7",
      "1c4bc1c6d3754cbab2e1e34c48140ef9",
      "98f320bd061144a6923bb7c8fd84083f",
      "a8368e2c8cb9479e93b9894a95cda6dd",
      "d7f1b2a94a4241b9a3435d83ea9e682b",
      "0fc55dd5778047498d55bcdc524f8144",
      "e1e8be2cd4814bde8665bcf4453e5de1",
      "ee8b868cd4d74f7d8ad3cbdf9d512020",
      "f41b627547d94121b9bbfd66fea22b31",
      "4b12d77e00374954acfb89f631a5cd8e",
      "a3a602cc7a8649439b793300e82f00e0",
      "3f316619bc89441fa6187aa27e194afa",
      "bc16738e2d174ee2997535c46a8bbc07",
      "dc5755c0f54a4d2d8fd175bf8d3de0f7",
      "92fe0ad578f64685ac535724a7d9d5b4",
      "113486e5a9314d6ca6154d47183d5e4b",
      "c386d68a181449a295ea99427108466c"
     ]
    },
    "id": "eM0OrBJbKoE-",
    "outputId": "714917c2-faf2-492a-eb4c-e0be0271f77d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918e2c14129747819503ec84827368c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usmpm87Dw66n"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/afan-oromo-ner/afri-berta-large-NER-afan-oromo\",               # Directory to save the model\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "58a7d8d4c44945899c51999e28dcc415",
      "293f759b138c40608daa0829553d04c8",
      "a2cc068d71f040c3998543a9964eb689",
      "364a0dc7d64d4af5b2e12bf54d9e9860",
      "1318f254fa7b43ccbcc29274aa20cb30",
      "31913ec9fc004444bbe862e19aabe5da",
      "c5cc9765b4c34f1e986a511aff64487f",
      "d3bd745a135b4e9893b00bad6f8620f0",
      "90ed8d04dbdc427395ed54832f5bed2a",
      "93a35deeb3da4e0ead72fba072a0d6ab",
      "0d360b4be5a54ea7bc1a2f30e81db116"
     ]
    },
    "id": "WtT0z_kUw_r2",
    "outputId": "16cc3942-f590-43c6-83fe-935a4d88455a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a7d8d4c44945899c51999e28dcc415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/503M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at castorini/afriberta_large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tXFI8KOSxCZR",
    "outputId": "5388c47d-789f-4911-d5da-826b30072bcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-19-1677865715.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mresearchmt12\u001b[0m (\u001b[33mresearchmt12-addis-ababa-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250621_202751-qzgznvqn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/qzgznvqn' target=\"_blank\">/content/drive/MyDrive/afan-oromo-ner/afri-berta-large-NER-afan-oromo</a></strong> to <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/qzgznvqn' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/qzgznvqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='274' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [274/380 02:26 < 00:56, 1.86 it/s, Epoch 3.59/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.338860</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>0.496753</td>\n",
       "      <td>0.495948</td>\n",
       "      <td>0.891837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.260156</td>\n",
       "      <td>0.569061</td>\n",
       "      <td>0.668831</td>\n",
       "      <td>0.614925</td>\n",
       "      <td>0.916327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.239004</td>\n",
       "      <td>0.589532</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.637854</td>\n",
       "      <td>0.926531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [380/380 03:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.338860</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>0.496753</td>\n",
       "      <td>0.495948</td>\n",
       "      <td>0.891837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.260156</td>\n",
       "      <td>0.569061</td>\n",
       "      <td>0.668831</td>\n",
       "      <td>0.614925</td>\n",
       "      <td>0.916327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.239004</td>\n",
       "      <td>0.589532</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.637854</td>\n",
       "      <td>0.926531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.239272</td>\n",
       "      <td>0.614085</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.657617</td>\n",
       "      <td>0.927755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.236368</td>\n",
       "      <td>0.597765</td>\n",
       "      <td>0.694805</td>\n",
       "      <td>0.642643</td>\n",
       "      <td>0.926122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=380, training_loss=0.2891856802137274, metrics={'train_runtime': 240.8298, 'train_samples_per_second': 25.018, 'train_steps_per_second': 1.578, 'total_flos': 328026936998400.0, 'train_loss': 0.2891856802137274, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "Dyq3W_lx5_kv",
    "outputId": "64dd76ab-a67d-4365-d80c-100c694c083f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18775728344917297, 'eval_precision': 0.667458432304038, 'eval_recall': 0.7805555555555556, 'eval_f1': 0.7195902688860435, 'eval_accuracy': 0.9409420289855073, 'eval_runtime': 0.967, 'eval_samples_per_second': 156.151, 'eval_steps_per_second': 10.341, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326,
     "referenced_widgets": [
      "6f01e100874943b381650372652903ff",
      "b568d910c4b2407e981a0477f26ac6a5",
      "22a99b3b7daa4c44b31345c39b1b8d85",
      "c203a6c20bf5415a87dfd82662e1a10d",
      "83bfeea4727247ba981b12767db641fa",
      "5bb4f75d2d0849ed9539b5923238803e",
      "a690a8fc913c4ef3b56d3cc7ab40426b",
      "d620afc8bbb34b35b3ef980a928f8e69",
      "14cd7d6c596d42b0b59bfd65edd42f6e",
      "4be9ea71fae5429bb66f7a15fd31612c",
      "3ad0aef3350e4cfa8abee26d21ca6e56",
      "191876ddcae9466586da94407dd356f2",
      "5ec26977f6d14e02b583ac9e6d2db42b",
      "f03d0dc1e2204cf89f9eb7e9a7b86f49",
      "366b4e3051a945e4abd714a4a61ddb0e",
      "dd82ba58d9064f72a9bbc9031d5cea61",
      "7d251fb997a84627b68a2e9acf9b3a70",
      "f93eb0803c424a85ac709cbc16f9da8b",
      "71fb69f3d13c4981802c3ed920115f22",
      "cc2faa8bf65d428bade755a8814a9a45",
      "5172959202f74b1fa57129c4495e5b76",
      "53f208a2e38c472cadd484e23f1035dc",
      "850bd47add5b4f0082f78fcd171df15c",
      "33784c543c594e75b8bcc7dbcd552377",
      "d694b388566f446699821bd3069fac80",
      "05d8147400d34d8cae89d7a915536aca",
      "eef7c92b0aac4dc48f2c6388f02f0db3",
      "8a10ccd93b6243588ba1c4a5a5357e33",
      "ebf7db253f8b4f1f88b9365a43281499",
      "98d5b48745b74895b4185bfacf3531ce",
      "9dc3c6007aee4a6bbe59b7366b62490a",
      "9b2c2780c50346c2a0c3a7144989587f",
      "f53927916c4a4110953dd830a0194bda",
      "31941d7c137a41ef88ed672d9ea2a542",
      "c27183719c55425797f09a4908284f64",
      "841ea8cfc4414dbfbcedec82c964ef6c",
      "b767c8769b444591ba92f5c51d15443c",
      "ab5dec8bc1294de8b32b9e8b01c25d4b",
      "3c000fd883634032959a4cf6758054e1",
      "8075c50bdd0947f08bd0add9fd9e4786",
      "d495e274d00347aabfc60c1cdfc3c964",
      "9d53d1fec99544358f3696fdeb649286",
      "4a5403e9ea224b29b5695a64d2b16006",
      "2286924256644a2e8ec707eb87170d83"
     ]
    },
    "id": "x1A6kaU07-tc",
    "outputId": "585845be-cbdf-4336-e362-08c02f59b96e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f01e100874943b381650372652903ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191876ddcae9466586da94407dd356f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850bd47add5b4f0082f78fcd171df15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31941d7c137a41ef88ed672d9ea2a542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/500M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Elu-dan/afri-berta-large-NER-Afan-oromo/commit/faee5d6ec653220b71e95a7b8ffbd1e6b5010379', commit_message='Training complete', commit_description='', oid='faee5d6ec653220b71e95a7b8ffbd1e6b5010379', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Elu-dan/afri-berta-large-NER-Afan-oromo', endpoint='https://huggingface.co', repo_type='model', repo_id='Elu-dan/afri-berta-large-NER-Afan-oromo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXujbp5q3QV6",
    "outputId": "93951ece-b5d4-4248-ab9b-ff2f1eabcd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LOC', 'score': np.float32(0.6469256), 'word': 'Xaaliyaanii', 'start': 18, 'end': 30}, {'entity_group': 'PER', 'score': np.float32(0.9499381), 'word': 'Joo', 'start': 30, 'end': 34}, {'entity_group': 'PER', 'score': np.float32(0.8997798), 'word': 'rijiyaa Melonii', 'start': 34, 'end': 49}, {'entity_group': 'LOC', 'score': np.float32(0.90984774), 'word': 'Liibiyaa', 'start': 56, 'end': 65}, {'entity_group': 'ORG', 'score': np.float32(0.862312), 'word': 'yakkaa Addunyaa ICC', 'start': 96, 'end': 116}]\n"
     ]
    }
   ],
   "source": [
    "result = token_classifier(\"Ministirri Muummee Xaaliyaanii Joorijiyaa Melonii lammii Liibiyaa yakka waraanaatiin mana murtii yakkaa Addunyaa ICC’n barbaadamu hidhaa gadhiisuun qorataman.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMkd6JGsWfuG"
   },
   "source": [
    "# **HornMT Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVXe3UfbU--W"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2T3EwUMTjuh",
    "outputId": "2f258bdc-dbcc-4543-e511-f2297c310891"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afri-berta-large-NER-Afan-oromo\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Afan-oromo-ner/orm.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Qx9NMAyTmV-",
    "outputId": "468bc345-3be0-4887-a43e-cece33bc3641"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2030 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing:   0%|          | 10/2030 [00:00<00:52, 38.18it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing: 100%|██████████| 2030/2030 [00:24<00:00, 84.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1fE38BRTte_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/hornmt_ao.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIE491KkTuT1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/hornmt_ao.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWV7AxgwTxAw",
    "outputId": "8ca69580-8c73-47fa-fd8f-1746d45f825b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/hornmt_ao.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u16VnstKT-Yx",
    "outputId": "f9d73baf-c5bf-4316-9a7e-2076ed86737f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 1750 'PER' entities. Saved to /content/drive/MyDrive/hornmt_per_entities_afanoromo.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/hornmt_ao.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/hornmt_per_entities_afanoromo.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWS3OOMWjPl"
   },
   "source": [
    "# **Flores 1 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as6g55OtXC0N",
    "outputId": "8c089424-9247-4698-bfe2-e318a5a5163d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Elu-dan/afri-berta-large-NER-Afan-oromo\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Afan-oromo-ner/orm.devtest\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JumUGERrXCpS",
    "outputId": "0c0f5a37-0216-4706-ca88-ccbfc1ff786d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1012 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing:   1%|          | 7/1012 [00:00<00:43, 23.18it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing: 100%|██████████| 1012/1012 [00:12<00:00, 83.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtzDMVn7XCd1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/flores1_ao.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acDwAsUxXCTZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/flores1_ao.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVtTUbc8XCLE",
    "outputId": "221efe20-41e5-44ea-8a0d-0b6d5c16faf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores1_ao.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQXADbhCXCDS",
    "outputId": "9e6eaf5a-1577-4c05-fbcd-e0d609203447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 396 'PER' entities. Saved to /content/drive/MyDrive/flores1_per_entities_afanoromo.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/flores1_ao.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores1_per_entities_afanoromo.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwhR5HZ41V42"
   },
   "source": [
    "# **Flores 2 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWAR47FqYbKY",
    "outputId": "e3e3773e-a036-4d21-ccbc-8818078213c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afri-berta-large-NER-Afan-oromo\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/Afan-oromo-ner/orm.dev\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JE3hdeNbYa6_",
    "outputId": "ca83e65c-93c5-430e-ddab-e72a3ab19b03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/997 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing:   1%|          | 10/997 [00:00<00:24, 40.77it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing: 100%|██████████| 997/997 [00:12<00:00, 80.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqO_VuE-Yawl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/flores2_ao.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OueigoRcYamK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/flores2_ao.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGSsPpJDYadk",
    "outputId": "b379e8a1-8d5a-4ccd-b177-94d7da1b51ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores2_ao.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzGWbnFGYaWQ",
    "outputId": "7c6b6b52-ec94-4145-dce4-0d7503188d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 411 'PER' entities. Saved to /content/drive/MyDrive/flores2_per_entities_afanoromo.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/flores2_ao.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores2_per_entities_afanoromo.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIA-8utk4gBt"
   },
   "source": [
    "# **NLLB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hipnMQwl4BlS",
    "outputId": "a919ef5a-94cf-4c6a-9abc-a944f1285c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1400000 sentences to /content/drive/MyDrive/om_final.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define file paths\n",
    "input_file = '/content/drive/MyDrive/om.txt'\n",
    "output_file = '/content/drive/MyDrive/om_final.txt'\n",
    "\n",
    "# Define the target number of sentences\n",
    "target_sentences = 1400000\n",
    "\n",
    "# Read the large dataset\n",
    "with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    all_sentences = f.readlines()\n",
    "\n",
    "# Randomly select 1.4 million sentences\n",
    "selected_sentences = random.sample(all_sentences, target_sentences)\n",
    "\n",
    "# Write the selected sentences to a new file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(selected_sentences)\n",
    "\n",
    "print(f\"Successfully saved {target_sentences} sentences to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euBrgVj9phCH"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_file = \"/content/drive/MyDrive/om_final.txt\"\n",
    "output_file = \"/content/drive/MyDrive/om_final_cleaned_dataset.txt\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        # Remove all characters except letters and spaces\n",
    "        clean_line = re.sub(r'[^A-Za-z\\s]', '', line)\n",
    "        # Optionally remove extra spaces and strip leading/trailing whitespace\n",
    "        clean_line = re.sub(r'\\s+', ' ', clean_line).strip()\n",
    "        outfile.write(clean_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW2WfstDxQo4"
   },
   "outputs": [],
   "source": [
    "# Efficient version for large files\n",
    "def split_file_into_chunks(file_path, output_prefix, num_chunks):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    chunk_size = total_sentences // num_chunks\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size if i != num_chunks - 1 else total_sentences\n",
    "        chunk = sentences[start:end]\n",
    "\n",
    "        with open(f\"{output_prefix}_chunk_{i+1}.txt\", 'w', encoding='utf-8') as out_f:\n",
    "            out_f.writelines(chunk)\n",
    "\n",
    "# Example usage:\n",
    "split_file_into_chunks(\"/content/drive/MyDrive/om_final_cleaned_dataset.txt\", \"/content/drive/MyDrive/split_oromo_files\", 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_SbMpV7W76c",
    "outputId": "80cac301-dff9-48bd-9c36-50c087be1673"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved human-readable NER output to: /content/drive/MyDrive/output_15_raw_orm.txt\n",
      "✅ Extracted 22737 PER entities (including duplicates) to: /content/drive/MyDrive/output_15_res_orm.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# File paths\n",
    "input_path = \"/content/drive/MyDrive/split_oromo_files/split_oromo_files_chunk_15.txt\"\n",
    "output_path = \"/content/drive/MyDrive/output_15_res_orm.txt\"\n",
    "raw_output_path = \"/content/drive/MyDrive/output_15_raw_orm.txt\"  # Now saving readable text\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint = \"/content/drive/MyDrive/afri-berta-ao-large/checkpoint-380\"\n",
    "\n",
    "# Initialize the pipeline\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=checkpoint,\n",
    "    tokenizer=checkpoint,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Entity merge logic (no '##' assumption)\n",
    "def merge_entities(ner_results):\n",
    "    merged = []\n",
    "    if not ner_results:\n",
    "        return merged\n",
    "\n",
    "    current = ner_results[0].copy()\n",
    "\n",
    "    for entity in ner_results[1:]:\n",
    "        if entity[\"entity_group\"] == current[\"entity_group\"] and entity[\"start\"] == current[\"end\"]:\n",
    "            current[\"word\"] += entity[\"word\"]\n",
    "            current[\"end\"] = entity[\"end\"]\n",
    "            current[\"score\"] = max(current[\"score\"], entity[\"score\"])\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            current = entity.copy()\n",
    "\n",
    "    merged.append(current)\n",
    "    return merged\n",
    "\n",
    "# Read input lines\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "per_entities = []\n",
    "\n",
    "# Open human-readable raw output file\n",
    "with open(raw_output_path, \"w\", encoding=\"utf-8\") as raw_out_f:\n",
    "    for idx, line in enumerate(lines):\n",
    "        try:\n",
    "            raw_output = token_classifier(line)\n",
    "\n",
    "            # Write plain readable results\n",
    "            raw_out_f.write(f\"[Line {idx+1}] {line}\\n\")\n",
    "            for ent in raw_output:\n",
    "                raw_out_f.write(\n",
    "                    f\"  - {ent['entity_group']}: {ent['word']} \"\n",
    "                    f\"(score={ent['score']:.2f}, start={ent['start']}, end={ent['end']})\\n\"\n",
    "                )\n",
    "            raw_out_f.write(\"\\n\")\n",
    "\n",
    "            # Merge and extract PER entities\n",
    "            merged_output = merge_entities(raw_output)\n",
    "            per_entities.extend([e[\"word\"].strip() for e in merged_output if e[\"entity_group\"] == \"PER\"])\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Line {idx+1} failed: {e}\")\n",
    "\n",
    "# Write extracted PER entities to output file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Saved human-readable NER output to: {raw_output_path}\")\n",
    "print(f\"✅ Extracted {len(per_entities)} PER entities (including duplicates) to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqCrxiU_wBvO",
    "outputId": "a4cc190d-1cb5-4d1d-eea1-912ee5b15901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete! Output file: /content/drive/MyDrive/afaoromo_merged_file1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the folder where your .txt files are located\n",
    "folder_path = '/content/drive/MyDrive/oromo'\n",
    "\n",
    "# List all .txt files in the folder\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Sort files if needed (optional)\n",
    "txt_files.sort()\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(folder_path, '/content/drive/MyDrive/afaoromo_merged_file1.txt')\n",
    "\n",
    "# Merge all the files\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for fname in txt_files:\n",
    "        file_path = os.path.join(folder_path, fname)\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(\"Merge complete! Output file:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdr_QfpNZ6sW"
   },
   "source": [
    "Merge File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMtv6F1PX_Xe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'path/to/your/folder'\n",
    "\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "txt_files.sort()\n",
    "\n",
    "output_file = os.path.join(folder_path, 'afaoromo_merged_file.txt')\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for fname in txt_files:\n",
    "        file_path = os.path.join(folder_path, fname)\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            outfile.write('\\n')  # Optional: adds a newline between files\n",
    "\n",
    "print(\"Merge complete! Output file:\", output_file)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
