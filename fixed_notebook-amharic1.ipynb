{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EO8ZkQHi4lRy",
    "outputId": "59c7236c-2be5-495c-ea24-e82a218700b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "dc43b4d5dbbf40258a80ced88232c44e",
       "pip_warning": {
        "packages": [
         "datasets",
         "fsspec"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "bca4b459f10041febf5a41d1c9346be5",
      "81ac4438b7934056aa484b80fc5e8a63",
      "92ae33706a1843a08dce13e02099499f",
      "5ce12acf73d54597a17fd0f264d26d8e",
      "5c91211292b3463288f4d7b6723c9373",
      "56b4b9558c0645b89f2ed4ff862cdca4",
      "9709a987a8074473ac1ab846026b3799",
      "6163396493444d23a639a73c65cc89ba",
      "d36c69a5fe6540fd886e5ca72930b88f",
      "292afecc89204782bf3ac73ce0b9f76f",
      "94b0e6cfdd8a4e21b026fd863a81324e",
      "40d00f5480694497afafdfc2f9c44eae",
      "9f62a56a6d134bb2b67ad48169ed570f",
      "3fe25050c8f4427a9daf3d12e345f77c",
      "b439b2d3acd141408e4bb810e12935dc",
      "f9b77f3fa577496ab5de695a39fda106",
      "d818016ae6ff4be1916d30f0d0c046b2",
      "65d722e8512540f4832fc5d87a6beb9a",
      "afba4f7598bf4a37b35d10869df63b88",
      "80e236e9b2d04d968d91046bf2e11305",
      "e1d2dc50edd5457fbb3cc393f036281c",
      "7c1e091dd2ab44ea80e41905b73f399a"
     ]
    },
    "id": "nqR1989m3glL",
    "outputId": "e0c51550-ba19-4044-ac1e-a15178078816"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca4b459f10041febf5a41d1c9346be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d00f5480694497afafdfc2f9c44eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "masakhaner.py:   0%|          | 0.00/7.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('masakhaner', 'amh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iz-7pZpN4Zyz",
    "outputId": "f999c0ef-68db-4d34-e987-29e594f17b5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1750\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzvDxF4i5j6a",
    "outputId": "72a8f662-c8c3-47b8-f134-5ac3a2db67e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ቀዳሚው',\n",
       " 'የሶማሌ',\n",
       " 'ክልል',\n",
       " 'በአወዳይ',\n",
       " 'ከተማ',\n",
       " 'ለተገደሉ',\n",
       " 'የክልሉ',\n",
       " 'ተወላጆች',\n",
       " 'ያከናወነው',\n",
       " 'የቀብር',\n",
       " 'ስነ',\n",
       " 'ስርዓትን',\n",
       " 'የተመለከተ',\n",
       " 'ዘገባ',\n",
       " 'ነው',\n",
       " '።']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSe9Elam5nlH",
    "outputId": "86080c6b-daf9-45a6-be2e-70e4b2b73ee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z53cFCIK5qj-",
    "outputId": "234d1ca6-946b-48bd-deef-eb46c551de63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = data[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhdvCLNH5tWE",
    "outputId": "3b98dea3-5e81-40e3-d81f-e2d533956de0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = data[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3TFqtqn5wvs",
    "outputId": "7ad44908-99cc-4278-9093-4866ba62943d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tQftaBW5zxP",
    "outputId": "f0eb6866-053d-4859-d3a8-61110e785a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ቀዳሚው የሶማሌ  ክልል   በአወዳይ ከተማ   ለተገደሉ የክልሉ ተወላጆች ያከናወነው የቀብር ስነ ስርዓትን የተመለከተ ዘገባ ነው ። \n",
      "O    B-LOC I-LOC I-LOC I-LOC O     O    O     O      O    O  O     O      O   O  O \n"
     ]
    }
   ],
   "source": [
    "words = data[\"train\"][0][\"tokens\"]\n",
    "labels = data[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "3e3ff91ab81146e9a05fb2809f4d5e10",
      "1d5c06ac302f4d4daca9207a2286e0c3",
      "e7a78cd9f823467ba71ca8beea135c26",
      "36b630c3d0234cb2b7ce9ac6919621a4",
      "8890f7c098b54398b2683b262abe73f8",
      "f3444e0b7bfb4fd7b809f08732db74cd",
      "b27d9192882d48208eabd810eb4fe7d6",
      "ca86031c44bf421889ff51422968e8e9",
      "25fac3b049f34b419f5157ea034f8358",
      "529a5bd2f3224d57a3d885b2662f0eb9",
      "41126e633c9d4b6fb75f7d4659e476f3",
      "26149e980a6a4e7bbd1d7ab8174d3872",
      "5c422fcd91df40f2a34e5f36506f9a88",
      "ca054fc2aecc45a5ae1121cb298d232e",
      "aa993e5494c7433db38a0e9349f582e5",
      "8353ca23f40f465b9784a3f9ee597cc0",
      "9bdbed719d174bd5acba8e01a2c8e5ea",
      "f8f6269833de4ff0bea1709d2b3191af",
      "298abffb40ec4c4cb3f6a61cf7ecf98e",
      "1bbeec618d7346548953480126fe40b0",
      "e52845f0eca14aff9a8cff5574b63e4d",
      "265b3f934d4742b5b66325918d5eeb42",
      "eca8bb5cf79f4dddb91ea6b6dcaae357",
      "d29aae69dcbc4dbbb88a546f650e0766",
      "05b374fbb0c64fb8b820eaf4f3c4ec90",
      "40179dcf4ed64c348df871958bec27ec",
      "845fb2d53eb44116ac76c2a3c61f081c",
      "64b59561f6244da6afbb0a07becbf1b6",
      "a16bf1c8e8f34245b1b2a99f93806f2c",
      "fc63b3f4ff4a4ea6b09ef17ab9492283",
      "6bc6e8cd3a0b4067ae488ce68c7614b3",
      "478264e64fc64a12a3eb725b802ac58c",
      "41adb6ae707f49cab5f209000926b281",
      "53b16d3eac864bcb978e3cc24bb668c6",
      "bbea0c51e34d4e00aaea72b51049cce6",
      "76cab553315449a0928de2f2cde31fcb",
      "269fae0402ab48d4b302b565f86a3464",
      "f047d9bfffda4e73b0752a0878b9cb0e",
      "39879a434f9c42bfb3ecebaa5dca8cb2",
      "14c42aad28e84782894bda89f7fc5292",
      "897961ca3bde44069c7ff851f9ab8264",
      "c984322d22b84d498daa471dc903d0d2",
      "20f425e48ced4d82addd900215580e7f",
      "9a4089c6029d49218e8ec05015a6712a"
     ]
    },
    "id": "C3jvi-f753JI",
    "outputId": "5a320c04-f445-446d-d27e-905f60c5b97c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3ff91ab81146e9a05fb2809f4d5e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26149e980a6a4e7bbd1d7ab8174d3872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca8bb5cf79f4dddb91ea6b6dcaae357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b16d3eac864bcb978e3cc24bb668c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDpeRHc37hOc",
    "outputId": "73e1f036-ae60-4f23-e612-40d4f727d406"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtqZNHh78ZFv",
    "outputId": "119c453b-0f00-434d-f810-e1447b6832aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁',\n",
       " 'ቀዳሚ',\n",
       " 'ው',\n",
       " '▁የ',\n",
       " 'ሶማሌ',\n",
       " '▁ክልል',\n",
       " '▁በአ',\n",
       " 'ወ',\n",
       " 'ዳይ',\n",
       " '▁ከተማ',\n",
       " '▁ለ',\n",
       " 'ተገደሉ',\n",
       " '▁የክልሉ',\n",
       " '▁ተወላጆች',\n",
       " '▁ያ',\n",
       " 'ከና',\n",
       " 'ወ',\n",
       " 'ነው',\n",
       " '▁የቀ',\n",
       " 'ብር',\n",
       " '▁ስነ',\n",
       " '▁ስርዓት',\n",
       " 'ን',\n",
       " '▁የተ',\n",
       " 'መለከተ',\n",
       " '▁ዘገባ',\n",
       " '▁ነው',\n",
       " '▁።',\n",
       " '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(data[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4WOPkvx8bgz",
    "outputId": "190a4b48-2354-4b81-e959-781c9bd5f5be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0tB-JZiS8efp"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDEBU50m8i5D",
    "outputId": "9d4f2ea6-f08a-4b49-af9e-25d625244991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = data[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ruBcW5RW8mRu"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d27a4f2dba4b4d36ade72d1a18910b9a",
      "fa4e9f57e153444ba23ffb2f71a64b08",
      "b4af2ef70c5b413592119b8eefdbe943",
      "d30e8ecec4814c3c94690714a4f15754",
      "ef3129e28cc444a5afa4030a390a80ab",
      "6a2c3bc06f2f40efa7a733ff845342ad",
      "ba1996fdd39943c6b1f980c39c2abf9f",
      "e569ec0df0024ae8956d5ff0eff79471",
      "ca5ff27da94e4edabadcf67000360306",
      "28007859cf1f43698286ba375610593a",
      "586257584e1740b4a61ce8eba7e5ac56",
      "d716fb454d8547de9aa270fa2c5f88b7",
      "9737b35f188f4c17a46df346f0aa5503",
      "261a255de09c4f21b07b69365daa1886",
      "1251229c205c43fd8fc35e1a4bd225f0",
      "a02ceb3942b1487b8ec3c2e11640abb5",
      "b18d16eff2144cc08f1cc319badb4ae1",
      "cb8ee5994b824a049c56f7aeebc2a28b",
      "4c4add298f114eff9b4755a233f3918f",
      "93e557fe22904cd4b4df2afda9713064",
      "11ff0d66f3ce45ed92c5b63cc476d419",
      "f2221ab2a49a4520b652989973fce38f",
      "ff173c7214e94f36b89a34ace4fcfb00",
      "f52cf1df88434960bcc47ed0ef9fc1a4",
      "dec776a4c6c248cebf66070b701b0371",
      "628b926f3c364cfa8d5fd4422bef12cc",
      "bd4405b6f42943ef81173eea08861cd4",
      "1dd158d50dd344a2b6fe4a7f82dfeebd",
      "6fd9d105961041da91255e27eeb586db",
      "89b1aaa277524a25b8241832ddb047dc",
      "c4969ab24b664868bdbf49c44bf9048d",
      "c432e49f028b44449d21d1b7420d9c6f",
      "79f0162e4a294a68b11bd55a022a3ced"
     ]
    },
    "id": "mwNi1qlH8pL9",
    "outputId": "6878dc3d-996b-4928-a029-42f7dda23542"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27a4f2dba4b4d36ade72d1a18910b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d716fb454d8547de9aa270fa2c5f88b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff173c7214e94f36b89a34ace4fcfb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bia9TMaw8wP7",
    "outputId": "7f6ed890-7519-4389-8c8d-f6261ba0bb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZY9Uu36k8sPL"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2T7xxQNe9EgX",
    "outputId": "b8a650a0-408b-43a0-b7ff-df37384c5559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    5,    6,    6,    6,    6,    6,    6,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0, -100, -100],\n",
       "        [-100,    3,    4,    4,    7,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    3,    4,    4,    4,    4,    4,    4,    4,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, -100]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8MJo45U9IR4",
    "outputId": "e7b8844f-a178-4af2-9c5a-53b7cb685ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 3, 4, 4, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g62DQKN9LSH",
    "outputId": "17f98ebd-1b6e-423d-fc2d-2faf4db4a759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=fa8590a8b8f9aa4602b7a5031901f52475c1f112ad019b41a87743389f0a659f\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD3prNHD_FCI",
    "outputId": "71fa297d-c914-456f-df52-fe045249bdcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "01259528d8d04492a428f384ac941106",
      "2b6971fb2f2544b1b4d6854f26657bf4",
      "0f06ce2d8ad947a3a30826478c53c2a7",
      "5c03d542bae34e9fb8db6b13e9e271d3",
      "51c081fea57540ebb22e9203d3cf3791",
      "02b325d80bc74e66b9daf550d410f078",
      "70d53765ad0f403b8863707584a42b5f",
      "7264fa80a6764842b3618a55ee09e6eb",
      "86e6c83211044d0482564f2ddd6a29c0",
      "efa791ae21024870824dc48d68c8c16e",
      "735758992ea24a9babf16181f76d3434"
     ]
    },
    "id": "gv0ZY55f_9t_",
    "outputId": "1e774e8d-be98-4c55-8b01-76c3343c4b12"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01259528d8d04492a428f384ac941106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpZErDGY9Ppb",
    "outputId": "a30e9d81-0889-4966-f761-9aab9593317d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': np.float64(0.0),\n",
       "  'recall': np.float64(0.0),\n",
       "  'f1': np.float64(0.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(0.0),\n",
       " 'overall_recall': np.float64(0.0),\n",
       " 'overall_f1': np.float64(0.0),\n",
       " 'overall_accuracy': 0.9375}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = 0  # Assuming 'O' corresponds to label index 0\n",
    "# Convert label indices to label names\n",
    "predictions = [label_names[p] for p in predictions]\n",
    "references = [label_names[l] for l in labels]\n",
    "metric.compute(predictions=[predictions], references=[references])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qMGoriAR9kku"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MV0n2u5e9nMb"
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137,
     "referenced_widgets": [
      "540dc859205c445a8d7216a3d4c090de",
      "86e29e12cc9b42518843298d960773eb",
      "c80beb505ae6485ab69dfffa828338fd",
      "93e5c7e4474e4cd08b87f5eb6203b704",
      "32a5dd509dd946b186a7e6da0441e59f",
      "0864e21547ff411da3334c4af84304c7",
      "98a509e388d54cdea827d2aa3e37963d",
      "071ad0186602436a839565faf29bafd4",
      "bdb6ac23bc0a447d89dda5008d2a91f7",
      "a780fabf0c534084871c1755496331d2",
      "9a1b30fe90714105ad94a61dd42a137f",
      "09588d8f914d499cbe88f029a2c492d8",
      "fa1bb382067842a7ae4fa5a8f5dd0766",
      "a7b41a63da2d42ddb0b80c1b719d95f7",
      "5e2aace062574ba68f6b294ecb0917d0",
      "9f1dc314f46c4e61b946ff9fa902a94b",
      "2f50b070963a4e5d92c67adce370d9c5",
      "a623390b4cb04104b91926b93e79bcbb",
      "b6138e858e4d4dc6ba9092463f11c484",
      "865b90e5867443c58b8468b9378751a9",
      "37eb7c5215c94868976d2799201015dc",
      "a2d74a21df184c7db7ec69f6736b9c65"
     ]
    },
    "id": "DE6Ckqr09pOH",
    "outputId": "36b3dff9-98aa-49ea-ba7a-a201f38c9ad4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540dc859205c445a8d7216a3d4c090de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09588d8f914d499cbe88f029a2c492d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "je4M2Dur9tD7",
    "outputId": "f3343621-ff9e-48d1-9210-c058e21b39d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "f669f301692c4f30ae6d9319ce77baac",
      "8791e5ba8cc64da0ac111d21f92f2114",
      "723b90f9e60047c09adf529a6c6a9cbc",
      "9d84e50971db4f02bc7374c7d7adf9ec",
      "7fe56b2ce6494cdfa07ea71821189cf0",
      "482ef56048f1483e8b2f1a237a3354ee",
      "ac2dda6bf8d045e39e651cf0aa207170",
      "f1db15c5c91641a3b7ed7555947887fa",
      "9e28965b8b824158802cbe673dd55826",
      "5bc7d614cdc44da9b48bb2f94eed0247",
      "7f03e55bb8914759abf7416c32a83549",
      "fb31b359c34d4d92b2fabb0c12f6d017",
      "01500739390845608c5e9ea77c2eb3a5",
      "6cbf479c43f44da39a9867e8780239d7",
      "07cf054ad070451f9bd80024a846772e",
      "6467133bc4f441998e98b4740f50e7dc",
      "a0e8a3c49a9044419aff2fd578c80e49",
      "671433d69cc14d6a8510d67d01047b20",
      "66d3f79696f44e4b998c6efdad9d8765",
      "4f36bf7d6be246f187f48fa602aa3b6d"
     ]
    },
    "id": "sg9d3ibJ92_q",
    "outputId": "8e79c064-b3ae-4ea2-b495-1f22084d6cc6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f669f301692c4f30ae6d9319ce77baac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FKjyAsDY95dc"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"/content/drive/MyDrive/afro-xlmr-base-finetuned-amharic-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "LynBxqJd-K5T",
    "outputId": "4916b7be-609d-4615-a7d0-fb233ca8d9da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-33-3203677919.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mresearchmt12\u001b[0m (\u001b[33mresearchmt12-addis-ababa-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250621_134952-nv8gfboz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/nv8gfboz' target=\"_blank\">/content/drive/MyDrive/afro-xlmr-base-finetuned-amharic-ner</a></strong> to <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/nv8gfboz' target=\"_blank\">https://wandb.ai/researchmt12-addis-ababa-university/huggingface/runs/nv8gfboz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [657/657 04:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.208251</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.599168</td>\n",
       "      <td>0.935122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.154812</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.672365</td>\n",
       "      <td>0.952612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.157177</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.740854</td>\n",
       "      <td>0.694286</td>\n",
       "      <td>0.955093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=657, training_loss=0.25086892358788615, metrics={'train_runtime': 285.8625, 'train_samples_per_second': 18.365, 'train_steps_per_second': 2.298, 'total_flos': 130970018456964.0, 'train_loss': 0.25086892358788615, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "uJPIU2i3zIFu",
    "outputId": "029df439-c073-42aa-c575-4c38bafe10fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18059279024600983, 'eval_precision': 0.6416275430359938, 'eval_recall': 0.7347670250896058, 'eval_f1': 0.6850459482038429, 'eval_accuracy': 0.9481296149386709, 'eval_runtime': 1.7021, 'eval_samples_per_second': 293.751, 'eval_steps_per_second': 37.013, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "d066e0d6301b4acd96cc1acd3a9b8a83",
      "a5093b77ae4d480c8b0af27b7d50013b",
      "776b7c8864d44a129645c507cb8392c3",
      "2fb59c889ab64597baf23fb31369dab1",
      "ffaaebf7d0b24e1b889dc543a99bdfc4",
      "ca0e956dec1c43db91d33cd9c8df649f",
      "11afabeb919c4c78bb6f3078bf6245cc",
      "52770897e46048c2a41f440efd54320b",
      "09be94c912a8484292d9444798042fb4",
      "790d6bf2ec7a48e99e0358ed30339c59",
      "6c474f95d3304cf8b9ba47670f9d470a"
     ]
    },
    "id": "yZS2JwSa-PMk",
    "outputId": "3584ee7f-e2a2-4b21-92aa-2c2705bd8046"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d066e0d6301b4acd96cc1acd3a9b8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1750514613.f6e7c05be0cb.2621.1:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Elu-dan/afro-xlmr-base-finetuned-amharic-ner/commit/cb3e3dfa7493c6fb6af93dc871ab75e298139f67', commit_message='Training complete', commit_description='', oid='cb3e3dfa7493c6fb6af93dc871ab75e298139f67', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Elu-dan/afro-xlmr-base-finetuned-amharic-ner', endpoint='https://huggingface.co', repo_type='model', repo_id='Elu-dan/afro-xlmr-base-finetuned-amharic-ner'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "3014927ba71c40f98662e6e51dc98f31",
      "e7c4f666b26545d1a929a19287763e81",
      "d79b57ffb96b42619312397b733d9442",
      "f95c528b716c46b89a8c29117e400b0b",
      "71993ccbfc14425382063f047ba697ef",
      "7b68db60becb4de7a17a6a0cd8e58a9f",
      "954cc807f3e34097a308bc8246cfc66a",
      "cfc717255fc5424395e5ebb3391fdd80",
      "8690232e1f20489ca7c6c40625d6eecf",
      "878b8795673247b2a3948576d41b8c9a",
      "9a368f7ad35b4385974b87378e6284cb",
      "033c17e40e2f432b878c065dce0b9136",
      "a0c9c92dec9d47dfa1459e48cd485b7b",
      "963235d7f985428a82f0986154655fa1",
      "5ef5fddb164e46dc8c91150baa222bda",
      "5cbf57b3c4af4b78af059a896e837e57",
      "06f3d1dfe41542a48120b01e71c8d8db",
      "07e3f33bf43a433ca0f96c08ae08633e",
      "13efb62920974e31939528b8b67a306a",
      "a9c40ec9a237499f8b9363499d027d98",
      "1bc753a3945c417c95b875f8da55ebb5",
      "5092dcd8fe934ae49cc63d64bc4fe97f",
      "0e6f933807494ca9bcebf1b3d5e4c673",
      "9442b2e06e364bf2a58776dd27ad9569",
      "38703ada727c41b48684aec06418f3f8",
      "555c25d2ae3a4c37881bcded0266d028",
      "7e5f50b0423041539d13707366503ba5",
      "3b3f55dc12ee4ddea5da0e8b08d075fb",
      "99561fbe95ec48b998dcdf09843b4631",
      "c9c698c0200d4683a5dd412725f4f6a9",
      "fb6884a1af8940429625a2b4cb8f9635",
      "d832375acfca41739b262fe6e793c0a7",
      "b59ef5fddbb04a8a9b06002352a706b5",
      "a1847810f8e6495e80ee1b9672823f97",
      "641d2d5a0bd848508ea16c1341551e86",
      "1921d57ef5b14fb7885c08e3a27845df",
      "b081a91046fb4035baea7c9d605673ce",
      "da9d1e3b836443d7a990ce245d5cec85",
      "a49a8024be97472dbed206370d606b24",
      "d93e458c0dba46199643208231e07777",
      "4537ef738c694289b260f451e0bad278",
      "a598245dcf2a4e2d89983c561c4e473d",
      "128fff0f3c6247d4a9c1d7f12f67a77b",
      "0fd4349efeb54ad191d1e4c4014811ba",
      "0e05d2d7cac8462e9428ea522717f657",
      "81b0adb35b004ac5af28318d02aba86b",
      "86a1c362b15946d5905df6e1e7a78ca4",
      "19a59f4e6d5642a2b309f9c338e08c32",
      "28f532f2527d4a93a293a428754eb186",
      "63b576c8a77c46398e6a052a7f779ba0",
      "e3342606c58b49e591a2bd6d2c920ba1",
      "d18b871b74e947f394ca47d09611d16d",
      "059584a24d3a493d88966b1b357d304c",
      "26c7887badd2451e93ec36e4288c1894",
      "86ca1cc13ed445f594f21c8cf921b3ae",
      "f287d453e6154906b2c361a1d4c2ce16",
      "46f4bb296800474ca82ab324776d9760",
      "1ef31c7f5a594349b717a7e9e6975978",
      "ab9bd1d5f3e8450a968b0528f657449d",
      "cb11b8cdace44693b696600b256b57f2",
      "ba948fbc96fe40ac9e9b55be633afab7",
      "756a6c388f75453a876d20107a9ea594",
      "ac3475f1bff3477c868f789fc55e0223",
      "c5502ff36254420e8709e1242cb424f6",
      "7cedbfe5554347f998692c3a18efe042",
      "9b222e779bc34155bf6cb85daea659a4"
     ]
    },
    "id": "0t4lXEId-u59",
    "outputId": "d4f9afdd-a4e1-44ce-9d8f-dd388801d169"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3014927ba71c40f98662e6e51dc98f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033c17e40e2f432b878c065dce0b9136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6f933807494ca9bcebf1b3d5e4c673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1847810f8e6495e80ee1b9672823f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05d2d7cac8462e9428ea522717f657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f287d453e6154906b2c361a1d4c2ce16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.89459324),\n",
       "  'word': 'ሳራ',\n",
       "  'start': 0,\n",
       "  'end': 2}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"ሳራ ጎበዝ ተማሪ ናት።\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYXjebm-JPKB",
    "outputId": "f66ba611-85d6-495b-efdb-226950a4d390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9738669),\n",
       "  'word': 'ኤልቤቴል ዳንኤል',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9394651),\n",
       "  'word': 'አዲስ አበባ',\n",
       "  'start': 18,\n",
       "  'end': 25},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.8600015),\n",
       "  'word': 'ድሬዳዋ',\n",
       "  'start': 29,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"ኤልቤቴል ዳንኤል የምትኖረው አዲስ አበባ እና ድሬዳዋ ነው።\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RZQdfzH12sGS"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/amh.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "\n",
    "texts = [text.strip() for text in texts if text.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98ev9vqp2u-p",
    "outputId": "244646ac-6a6c-4039-e8f8-4e4e2a657a31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 8/2030 [00:00<00:27, 73.37it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing: 100%|██████████| 2030/2030 [00:25<00:00, 79.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kE37VJcG2xie"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_amharic1.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fsyqNsbB22bw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_amharic1.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlMxY40223F6",
    "outputId": "f5aa66a8-ae7d-488d-bdc6-6d77fbf397e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/hornmt_ner_results_amharic1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEFzVUyi6qtJ",
    "outputId": "9c4f4d27-1ce8-41ad-bdc8-07cd697c5203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 907 'PER' entities. Saved to /content/drive/MyDrive/hornmt_per_results_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_amharic1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/hornmt_per_results_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7RT8rRDKJaf"
   },
   "source": [
    "# **Flores 1 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgczphj9B-0f",
    "outputId": "6e2bfcbb-ae65-44e1-fefd-269d78923e68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9RzCQpqCuj7s"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/amh.dev\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zKNwzquxlvJ",
    "outputId": "830f7e8b-0467-45ff-bc1c-d7bf48eb9a52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   1%|          | 5/997 [00:00<01:42,  9.67it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 997/997 [00:13<00:00, 73.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Oc--WcYAxm9B"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores1_amharic.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1LwPUo8axurT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores1_amharic.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haQ1P1F5xyuq",
    "outputId": "f3cddc8b-fcfc-4589-fc30-e2d914349018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores1_ner_results_amharic_new.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfzdaa6sx2m1",
    "outputId": "9b4f1131-77c5-47b3-d44c-514cde5e1402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 234 'PER' entities. Saved to /content/drive/MyDrive/flores1_per_results_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_flores1_amharic.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores1_per_results_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27bcFgAc87BO"
   },
   "source": [
    "# Flores 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3UeInEA86iD",
    "outputId": "a7c563dc-25ae-4897-a995-774a1e9b4ec8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ouUulFH9IsN"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/amh.devtest\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whQ4rVW69IpX",
    "outputId": "05789987-782a-497c-92ee-983c584b6d60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   1%|          | 10/1012 [00:00<00:38, 25.83it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 1012/1012 [00:30<00:00, 33.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KoVONhb79Il9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores2_amharic.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hwhttRJJ9Iip"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_flores2_amharic.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwEH6K_09adi",
    "outputId": "e3ab6e69-db6c-4404-ac4d-5cb4334400b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/flores2_ner_results_amharic_new.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYMqiASm9dRM",
    "outputId": "610fc54c-d7df-40e6-f73a-1484ade43976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 225 'PER' entities. Saved to /content/drive/MyDrive/flores2_per_results_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_flores2_amharic.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/flores2_per_results_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFvkyJ_QLLOX"
   },
   "source": [
    "# **Mafand 1 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNmRZ3jS_4Oh",
    "outputId": "069a9e1f-3013-42ef-9cda-9e4b79461d9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UVK5yYBQye_j"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/dev.amh\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fHmpkOe6B-b",
    "outputId": "943dc27e-6006-4db3-90d8-333ca7fdb9d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   1%|          | 10/899 [00:01<01:21, 10.85it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 899/899 [00:22<00:00, 39.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results1 = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result1 = token_classifier(text)\n",
    "    results1.append({\"text\": text, \"ner_results\": ner_result1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SfqmEDWv6IpQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_mafand.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results1, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "79KKd9go6Qar"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results1]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_mafand.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_iBjmx96Q-n",
    "outputId": "8b19fb5e-1b53-44c0-b8a0-8eab186c9d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/mafand_ner_results_amharic.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results1:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GJxmpRU6Vz-",
    "outputId": "d9d4606e-1f75-4c1c-9f22-1c8920e0720b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 356 'PER' entities. Saved to /content/drive/MyDrive/mafand_per_results_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_mafand.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/mafand_per_results_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-JCdKVnAQ80"
   },
   "source": [
    "# **Mafand 2 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CGHGxLm6Xk9",
    "outputId": "29989944-2f3c-4e94-8674-eb6367048645"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A-KnY7-qaJwK"
   },
   "outputs": [],
   "source": [
    "file_path = \"/content/drive/MyDrive/test.amh\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJE4GsMVAZdG",
    "outputId": "f5a27fda-82bf-4747-c996-5e5d2c2c8e33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   1%|          | 7/1037 [00:00<00:51, 19.87it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 1037/1037 [00:15<00:00, 68.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results1 = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result1 = token_classifier(text)\n",
    "    results1.append({\"text\": text, \"ner_results\": ner_result1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9SBQ1Ba2Afxz"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to make objects JSON serializable\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/ner_results_mafand2.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results1, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fUk0hDiEApbf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results1]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/ner_results_mafand2.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJUoWTvxAmjY",
    "outputId": "c48e82ec-62c2-491c-a68e-18a8c52849e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/mafand2_ner_results_amharic.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results1:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIH3d0_pAs0s",
    "outputId": "4aa5b2b7-86ee-4b64-82d7-1da198a26ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 381 'PER' entities. Saved to /content/drive/MyDrive/mafand2_per_results_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/ner_results_mafand2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/mafand2_per_results_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VrfpVbLOOUc"
   },
   "source": [
    "# **NLLB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BfsZ0QlMGW8",
    "outputId": "4107ca9d-c5c7-4910-a430-ee3b2690ca62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1400000 sentences to /content/drive/MyDrive/am_final.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define file paths\n",
    "input_file = '/content/drive/MyDrive/am.txt'\n",
    "output_file = '/content/drive/MyDrive/am_final.txt'\n",
    "\n",
    "# Define the target number of sentences\n",
    "target_sentences = 1400000\n",
    "\n",
    "# Read the large dataset\n",
    "with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    all_sentences = f.readlines()\n",
    "\n",
    "# Randomly select 1.4 million sentences\n",
    "selected_sentences = random.sample(all_sentences, target_sentences)\n",
    "\n",
    "# Write the selected sentences to a new file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(selected_sentences)\n",
    "\n",
    "print(f\"Successfully saved {target_sentences} sentences to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQ7l6ddHM5Pi",
    "outputId": "73ccfaec-7b9b-4e01-a9a0-0c22dd8201e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into 20 files in '/content/drive/MyDrive/split_files_amharic'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_text_dataset(input_file, output_dir, num_files=20):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines per file\n",
    "    total_lines = len(lines)\n",
    "    lines_per_file = total_lines // num_files\n",
    "    remainder = total_lines % num_files\n",
    "\n",
    "    # Split the lines into chunks\n",
    "    start = 0\n",
    "    for i in range(num_files):\n",
    "        end = start + lines_per_file + (1 if i < remainder else 0)\n",
    "        chunk = lines[start:end]\n",
    "\n",
    "        # Write the chunk to a new file\n",
    "        output_file = os.path.join(output_dir, f'output_{i+1}.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "            out_file.writelines(chunk)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    print(f\"Dataset split into {num_files} files in '{output_dir}'.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = '/content/drive/MyDrive/am_final.txt'  # Path to your input text file\n",
    "output_dir = '/content/drive/MyDrive/split_files_amharic'  # Directory to save the split files\n",
    "split_text_dataset(input_file, output_dir, num_files=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2DBBv9tNjjq",
    "outputId": "0871e39b-6182-4940-8eca-bd84740c7dcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Elu-dan/afro-xlmr-base-finetuned-amharic-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AcYOdpUPQGc8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQCWwOZGNjgh",
    "outputId": "99facd4e-c109-4d86-d7e0-1c28fbdbc99f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 10/70001 [00:00<29:14, 39.90it/s] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Sentences: 100%|██████████| 70001/70001 [13:33<00:00, 86.04it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/content/drive/MyDrive/split_files_amharic/output_20.txt\"  # Update with the actual path to the file\n",
    "\n",
    "# Read the file into a list of sentences\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    texts = file.read().splitlines()\n",
    "\n",
    "# Remove empty lines and strip extra spaces\n",
    "texts = [text.strip() for text in texts if text.strip()]\n",
    "\n",
    "results = []\n",
    "for text in tqdm(texts, desc=\"Processing Sentences\"):\n",
    "    ner_result = token_classifier(text)\n",
    "    results.append({\"text\": text, \"ner_results\": ner_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OoB0U-luNjXZ"
   },
   "outputs": [],
   "source": [
    "def make_serializable(obj):\n",
    "    if isinstance(obj, np.float32):  # Convert np.float32 to float\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):  # Convert np.ndarray to a list\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "# Save the results\n",
    "output_path = \"/content/drive/MyDrive/Amharic-NER/output_20_amharic.json\"  # Update with your desired path\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, default=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7aMSvJVJNjUz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [{\"text\": entry[\"text\"], \"ner_results\": entry[\"ner_results\"]} for entry in results]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "output_path = \"/content/drive/MyDrive/Amharic-NER/output_20_amharic.csv\"  # Update with your desired path\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob3vDrS_NjSN",
    "outputId": "f797c35e-cf52-41e0-a8b5-2ce651d1b47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/Amharic-NER/nllb_output20_amharic.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in results:\n",
    "        f.write(f\"Text: {entry['text']}\\n\")\n",
    "        f.write(f\"NER Results: {entry['ner_results']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Results saved to ner_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Edbucg8yNi_t",
    "outputId": "7c4b6559-42a7-4c00-a81c-f2de5f531dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 10521 'PER' entities. Saved to /content/drive/MyDrive/Amharic-NER/nllb_output20_per_amharic.txt.\n"
     ]
    }
   ],
   "source": [
    "# Load from the JSON file you saved\n",
    "with open(\"/content/drive/MyDrive/Amharic-NER/output_20_amharic.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ner_data = json.load(f)\n",
    "\n",
    "# Extract only \"PER\" entities\n",
    "per_entities = []\n",
    "for entry in ner_data:\n",
    "    per_entities.extend([ent[\"word\"] for ent in entry.get(\"ner_results\", []) if ent.get(\"entity_group\") == \"PER\"])\n",
    "\n",
    "# Save to text file\n",
    "output_file = \"/content/drive/MyDrive/Amharic-NER/nllb_output20_per_amharic.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in per_entities:\n",
    "        f.write(word + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(per_entities)} 'PER' entities. Saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktzwsPXXTxew"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
